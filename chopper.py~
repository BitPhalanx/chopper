# Attempt to play Chopper Command using OpenAI library
# There are 2 versions of the game
#
# 1. RAM as input (ChopperCommand-ram-v0)
#      RAM of Atari 2600 consists of 128 bytes
#      AI nets score higher using this as input
# 2. Screen images as input (ChopperCommand-v0)
#      RGB image, array of shape (210, 160, 3)
#
# Each action is repeatedly performed for k frames,
# with k being uniformly sampled from {2,3,4}
#
# Policy gradient is confusing and running out of time
# I will try to fit best episode per batch


# Import OpenAI gym and other needed libraries
import gym
import tensorflow as tf
import numpy as np
import random
import time

# Variable to set BatchNorm to train or not
# Set to false during verification run
bn_is_training = True

def cnn_model():
  # Batch Norm HyperParameters
  bn_scale = True

  input_tensor = tf.placeholder(tf.float32)

  # Input layer takes in 104x80x3 = 25200
  with tf.name_scope('reshape'):
    input_reshape = tf.reshape(input_tensor, [-1, 104, 80, 3])

  # Conv 3x3 box across 3 color channels into 32 features
  with tf.name_scope('conv1'):
    W_conv1 = weight_variable([3,3,3,32])
    b_conv1 = bias_variable([32])
    pre_bn_conv1 = conv2d(input_reshape, W_conv1) + b_conv1
    post_bn_conv1 = tf.contrib.layers.batch_norm(pre_bn_conv1, center = True, scale = bn_scale, is_training = bn_is_training, scope = 'bn1')
    h_conv1 = tf.nn.relu(post_bn_conv1)

  # Max pool to half size (52x40)
  with tf.name_scope('pool1'):
    h_pool1 = max_pool_2x2(h_conv1)

  # 2nd conv, 3x3 box from 32 to 64 features
  with tf.name_scope('conv2'):
    W_conv2 = weight_variable([3,3,32,64])
    b_conv2 = bias_variable([64])
    pre_bn_conv2 = conv2d(h_pool1, W_conv2) + b_conv2
    post_bn_conv2 = tf.contrib.layers.batch_norm(pre_bn_conv2, center = True, scale = bn_scale, is_training = bn_is_training, scope = 'bn2')
    h_conv2 = tf.nn.relu(post_bn_conv2)

  # 2nd max pool, half size again (26x20)
  with tf.name_scope('pool2'):
    h_pool2 = max_pool_2x2(h_conv2)

  # 3rd conv, 3x3 box from 64 to 128 features
  with tf.name_scope('conv3'):
    W_conv3 = weight_variable([3,3,64,128])
    b_conv3 = bias_variable([128])
    pre_bn_conv3 = conv2d(h_pool2, W_conv3) + b_conv3
    post_bn_conv3 = tf.contrib.layers.batch_norm(pre_bn_conv3, center = True, scale = bn_scale, is_training = bn_is_training, scope = 'bn3')
    h_conv3 = tf.nn.relu(post_bn_conv3)

  # 3rd max pool, half size last time (13x10)
  with tf.name_scope('pool3'):
    h_pool3 = max_pool_2x2(h_conv3)

  # First fully connected layer, 13*10*128 = 16640 to 512 fully connected
  with tf.name_scope('fc1'):
    W_fc1 = weight_variable([13*10*128, 512])
    b_fc1 = bias_variable([512])
    # Flatten max pool to enter fully connected layer
    h_pool3_flat = tf.reshape(h_pool3, [-1, 13*10*128])
    h_fc1 = tf.nn.relu(tf.matmul(h_pool3_flat, W_fc1) + b_fc1)

  with tf.name_scope('dropout'):
    keep_prob = tf.placeholder(tf.float32)
    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)

  # Fully connected from 512 to 6 (1 for each action possible)
  with tf.name_scope('fc2'):
    W_fc2 = weight_variable([512, 6])
    b_fc2 = bias_variable([6])

    y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2
    moveprobs = tf.nn.softmax(y_conv)
  
  #TODO Optimizer and training setup
  # loss = 
  # train = tf.train.AdamOptimizer().minimize(loss)

  return moveprobs, input_tensor, keep_prob

def conv2d(x, W):
  # Return full stride 2d conv
  return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')

def max_pool_2x2(x):
  # 2x2 max pool
  return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')

def weight_variable(shape):
  initial = tf.contrib.layers.xavier_initializer()
  return tf.Variable(initial(shape))

def bias_variable(shape):
  initial = tf.contrib.layers.xavier_initializer()
  return tf.Variable(initial(shape))

def choose_action(moveprobs, epsilon_greedy):
  # Feed in probability and return an action 
  # Actions: up, down, left, right, shoot, nothing
  #           2     5     4      3      1        0
  if np.random.uniform() <= epsilon_greedy:
    random_selection = random.randint(0,5)
    return random_selection
  else:
    index_of_max = moveprobs[0][0].argmax()
    return index_of_max
  
def prep_image(observation):
  # observation holds unsigned 8 bit int array
  # with shape (210, 160, 3). Half this
  reduced_observation = observation[::2, ::2, :]
  # Remove odd number from first observation
  reduced_observation = reduced_observation[1:, :, :]
  # reduced_observation is now shape (104,80,3)
  float_input = reduced_observation.astype(np.float32)
  return float_input
 
def main():
  # Parameters
  render_graphics = False
  slowdown_dbg = False
  learning_rate = 0.001
  epsilon_greedy = 1.0
  epsilon_decay = 0.999
  # num_of_batches = 500
  # episodes_per_batch = 10
  num_of_batches = 3
  episodes_per_batch = 5

  # Start the game
  env = gym.make('ChopperCommand-v0')
  observation = env.reset()

  # Prepare our CNN model and get first image
  sess = tf.InteractiveSession()
  float_input = prep_image(observation)
  moveprobs, input_tensor, keep_prob = cnn_model()
  sess.run(tf.global_variables_initializer())

  # Prepare game management variables

  for batch in range(num_of_batches):
    # Store array of moves and final score for each episode
    nn_input_arr, chosen_act_arr, rewards_arr, total_episode_reward_arr = [], [], [], []

    for episode in range(episodes_per_batch):
      # Per episode game training management variables
      ep_nn_input_arr, ep_chosen_act_arr, ep_rewards_arr = [], [], []
      episode_running = True
      episode_reward = 0

      while episode_running:
        ep_nn_input_arr.append(float_input)
        output_actions = sess.run([moveprobs], feed_dict={input_tensor: float_input, keep_prob: 0.5})
        chosen_act = choose_action(output_actions, epsilon_greedy)
        ep_chosen_act_arr.append(chosen_act)
        observation, reward, done, info = env.step(chosen_act)
        ep_rewards_arr.append(reward)
        episode_reward = episode_reward + reward

        # Prepare next image for input into our graph
        float_input = prep_image(observation)

        if slowdown_dbg:
          # Slowdown to better see what's happening
          time.sleep(0.05)
        if render_graphics:
          env.render()
        if done:
          # Store episode list to batch list
          nn_input_arr.append(ep_nn_input_arr)
          chosen_act_arr.append(ep_chosen_act_arr)
          rewards_arr.append(ep_rewards_arr)
          # Then reset the environment for next episode.
          print("  Total reward this episode: {}".format(episode_reward))
          total_episode_reward_arr.append(episode_reward)
          episode_reward = 0 # Probably not needed
          observation = env.reset()
          float_input = prep_image(observation)
          episode_running = False
    # Results from this batch
    print("Batch #{} has been completed.".format(batch+1))
    index_best_eps = total_episode_reward_arr.index(max(total_episode_reward_arr))
    frames_of_best_eps = len(chosen_act_arr[index_best_eps])
    print("Best episode was #{}.".format(index_best_eps+1))
    print("Number of frames seen this episode was {}".format(frames_of_best_eps+1))
    # nn_input_arr[episode][step_of_turn] is numpy with shape (104,80,3)
    # print("  Number of images seen on best episode was {}".format(len(nn_input_arr[index_best_eps])))
    # chosen_act_arr[episode][step_of_turn] is int holding action chosen that step of turn
    # print("  Number of actions chosen on best episode was {}".format(len(chosen_act_arr[index_best_eps])))
    # rewards_arr is list of int holding reward for that instance of input
    # print("  Number of individual rewards on best episode was {}".format(len(rewards_arr[index_best_eps])))
    # total_episode_reward_arr is list of int holding cumulative score of each episode
    # print("  Number of total episodes stored this batch is {}".format(len(total_episode_reward_arr)))

    #TODO Backpropagate here. Choose to propagate the best episode from batch
    # Perhaps instead of policy, use regression and fit input to output
    # Using nn_input_arr[index_best_eps] list of input numpy array
    # and chosen_act_arr[index_best_eps] list of output neurons

    # Also recalculate epsilon_greedy
    epsilon_greedy = epsilon_greedy * epsilon_decay
    print("Reduce epsilon_greedy to {}".format(epsilon_greedy))

  # Now that training batches are done, do test runs
  print("Batches complete, now we test the NN!")
  # Set batchnorm training to false
  bn_is_training = False
  for testcount in range(episodes_per_batch):
    print("Test run #{}".format(testcount+1))
    observation = env.reset()
    float_input = prep_image(observation)
    testloop = True
    episode_reward = 0
    while testloop:
      output_actions = sess.run([moveprobs], feed_dict={input_tensor: float_input, keep_prob: 1.0})
      chosen_act = choose_action(output_actions, 0.0)
      observation, reward, done, info = env.step(chosen_act)
      episode_reward += reward
      float_input = prep_image(observation)
      if done:
        testloop = False
    print("  Score: {}".format(episode_reward))



if __name__ == "__main__":
  main()
